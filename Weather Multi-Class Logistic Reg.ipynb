{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00900000e+03 9.30000000e+01 2.88270000e+02 1.00000000e+00]\n",
      " [1.00800000e+03 9.20000000e+01 2.88297576e+02 1.00000000e+00]\n",
      " [1.00800000e+03 9.00000000e+01 2.88334343e+02 1.00000000e+00]\n",
      " ...\n",
      " [1.02000000e+03 5.00000000e+01 2.96370000e+02 3.00000000e+00]\n",
      " [1.02000000e+03 4.90000000e+01 2.94650000e+02 2.00000000e+00]\n",
      " [1.02000000e+03 5.20000000e+01 2.91440000e+02 2.00000000e+00]]\n",
      "[['sky is clear']\n",
      " ['sky is clear']\n",
      " ['sky is clear']\n",
      " ...\n",
      " ['sky is clear']\n",
      " ['haze']\n",
      " ['haze']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import Counter \n",
    "import sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import operator\n",
    "from sklearn import linear_model\n",
    "import scipy.io\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "# run fmin on the loss function and gradient \n",
    "\n",
    "df1 = pd.read_csv(\"pressure.csv\")\n",
    "data1 = df1[['Houston']]\n",
    "data1 = np.array(data1)\n",
    "\n",
    "df2 = pd.read_csv(\"humidity.csv\")\n",
    "data2 = df2[['Houston']]\n",
    "data2 = np.array(data2)\n",
    "\n",
    "df3 = pd.read_csv(\"temperature.csv\")\n",
    "data3 = df3[['Houston']]\n",
    "data3 = np.array(data3)\n",
    "\n",
    "df4 = pd.read_csv(\"wind_speed.csv\")\n",
    "data4 = df4[['Houston']]\n",
    "data4 = np.array(data4)\n",
    "\n",
    "training = np.hstack(np.array([data1, data2, data3, data4]))\n",
    "\n",
    "\n",
    "training = training[1:,:]\n",
    "\n",
    "df5 = pd.read_csv(\"weather_description.csv\")\n",
    "data5 = df5[['Houston']]\n",
    "data5 = np.array(data5)\n",
    "\n",
    "#vector of weather classifications\n",
    "\n",
    "output = data5[1:,:] #45252\n",
    "\n",
    "print (training)\n",
    "\n",
    "print (output)\n",
    "#training = np.array(merge[:, :4])\n",
    "\n",
    "\n",
    "#output = merge[:, 4:]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"['sand']\": 0, \"['scattered clouds']\": 1, \"['thunderstorm']\": 2, \"['heavy intensity drizzle']\": 3, \"['thunderstorm with rain']\": 4, \"['broken clouds']\": 5, \"['mist']\": 6, \"['dust']\": 7, \"['sky is clear']\": 8, \"['overcast clouds']\": 9, \"['fog']\": 10, \"['proximity moderate rain']\": 11, \"['few clouds']\": 12, \"['shower rain']\": 13, \"['volcanic ash']\": 14, \"['light snow']\": 15, \"['thunderstorm with drizzle']\": 16, \"['squalls']\": 17, \"['light rain']\": 18, \"['proximity thunderstorm']\": 19, \"['proximity shower rain']\": 20, \"['proximity thunderstorm with rain']\": 21, \"['heavy intensity rain']\": 22, \"['thunderstorm with light drizzle']\": 23, \"['smoke']\": 24, \"['very heavy rain']\": 25, \"['thunderstorm with light rain']\": 26, \"['light intensity drizzle']\": 27, \"['heavy intensity shower rain']\": 28, \"['moderate rain']\": 29, \"['haze']\": 30, \"['thunderstorm with heavy rain']\": 31, \"['drizzle']\": 32}\n",
      "{0: \"['sand']\", 1: \"['scattered clouds']\", 2: \"['thunderstorm']\", 3: \"['heavy intensity drizzle']\", 4: \"['thunderstorm with rain']\", 5: \"['broken clouds']\", 6: \"['mist']\", 7: \"['dust']\", 8: \"['sky is clear']\", 9: \"['overcast clouds']\", 10: \"['fog']\", 11: \"['proximity moderate rain']\", 12: \"['few clouds']\", 13: \"['shower rain']\", 14: \"['volcanic ash']\", 15: \"['light snow']\", 16: \"['thunderstorm with drizzle']\", 17: \"['squalls']\", 18: \"['light rain']\", 19: \"['proximity thunderstorm']\", 20: \"['proximity shower rain']\", 21: \"['proximity thunderstorm with rain']\", 22: \"['heavy intensity rain']\", 23: \"['thunderstorm with light drizzle']\", 24: \"['smoke']\", 25: \"['very heavy rain']\", 26: \"['thunderstorm with light rain']\", 27: \"['light intensity drizzle']\", 28: \"['heavy intensity shower rain']\", 29: \"['moderate rain']\", 30: \"['haze']\", 31: \"['thunderstorm with heavy rain']\", 32: \"['drizzle']\"}\n",
      "[[8]\n",
      " [8]\n",
      " [8]\n",
      " ...\n",
      " [8]\n",
      " [30]\n",
      " [30]]\n"
     ]
    }
   ],
   "source": [
    "list1 = []\n",
    "\n",
    "for element in output:\n",
    "\tlist1.append(str(element))\n",
    "\n",
    "unique = list(set(list1))     #get list of unique weather classifications\n",
    "\n",
    "dict1 = {}                    #map each weather classification to an integer in between 0 and 32 (i.e its index)\n",
    "for i in range(len(unique)):\n",
    "\tdict1[unique[i]] = i\n",
    "\n",
    "print (dict1)\n",
    "dict2 = {}                    #map each integer from 0 to 32 to its weather classification (reverse of above for retreival later)\n",
    "for element in dict1.keys():\n",
    "    dict2[dict1[element]] = element\n",
    "\n",
    "print (dict2)\n",
    "\n",
    "rows = output.shape[0]\n",
    "columns = output.shape[1]\n",
    "\n",
    "\n",
    "for i in range(rows):         #convert each element in the training output vector to an integer from 0 to 32\n",
    "\n",
    "\toutput[i,:] = dict1[str(output[i,:])]\n",
    "    \n",
    "print (output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 1.00900000e+03 9.30000000e+01 ... 8.30995929e+04\n",
      "  2.88270000e+02 1.00000000e+00]\n",
      " [1.00000000e+00 1.00800000e+03 9.20000000e+01 ... 8.31154922e+04\n",
      "  2.88297576e+02 1.00000000e+00]\n",
      " [1.00000000e+00 1.00800000e+03 9.00000000e+01 ... 8.31366936e+04\n",
      "  2.88334343e+02 1.00000000e+00]\n",
      " ...\n",
      " [1.00000000e+00 1.02000000e+03 5.00000000e+01 ... 8.78351769e+04\n",
      "  8.89110000e+02 9.00000000e+00]\n",
      " [1.00000000e+00 1.02000000e+03 4.90000000e+01 ... 8.68186225e+04\n",
      "  5.89300000e+02 4.00000000e+00]\n",
      " [1.00000000e+00 1.02000000e+03 5.20000000e+01 ... 8.49372736e+04\n",
      "  5.82880000e+02 4.00000000e+00]]\n",
      "[8 8 8 ... 8 30 30]\n"
     ]
    }
   ],
   "source": [
    "X = np.nan_to_num(training) #convert all NaN to zero\n",
    "\n",
    "rows2 = X.shape[0]\n",
    "columns2 = X.shape[1]\n",
    "\n",
    "average_of_columns = np.sum(X, axis = 0)/rows2 #average all columns \n",
    "\n",
    "for i in range(rows2):                  #each zero number becomes the average of its column \n",
    "    for j in range(columns2):\n",
    "        if X[i,j] == 0:\n",
    "            X[i,j] = average_of_columns[j]\n",
    "    \n",
    "    \n",
    "p = 2\n",
    "poly = sklearn.preprocessing.PolynomialFeatures(degree=p,include_bias=False)\n",
    "X_poly = poly.fit_transform(X)\n",
    "XX = np.vstack([np.ones((X_poly.shape[0],)),X_poly.T]).T           #add a column of ones to the beginning of the training matrix\n",
    "\n",
    "#print (XX.shape)\n",
    "    \n",
    "y = np.nan_to_num(output)\n",
    "\n",
    "y = np.ndarray.flatten(y)\n",
    "\n",
    "average_of_y = np.sum(y)/len(y)\n",
    "\n",
    "for i in range(len(y)):             #each zero number becomes the average of the vector\n",
    "\n",
    "\tif y[i] == np.nan:\n",
    "\n",
    "\t\ty[i] = int(average_of_y)\n",
    "        \n",
    "print(XX)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def sigmoid (z):                      #sigmoid hypothesis function\n",
    "    sig = 1/(1+np.exp(-1*z))\n",
    "    return sig            \n",
    "        \n",
    "#print len(Counter(y))\n",
    "def find_theta(X_train, X_test, y_train):                #find the vector of thetas (33 total)\n",
    "    theta = []\n",
    "    \n",
    "\n",
    "  \n",
    "    X_train = 1 + np.log(X_train)  #scale features\n",
    "    \n",
    "    \"\"\"\n",
    "    mu = np.mean(X,axis=0)\n",
    "    sigma = np.std(X,axis=0)\n",
    "    X = (X - mu) / sigma\n",
    "    X[X == np.nan] = 1000\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    for i in range(len(unique)):             #the multi-class logistic regression will essentially run 33 binary logistic regresssions\n",
    "\n",
    "        y1 = copy.deepcopy(y_train)\n",
    "\n",
    "        for j in range(len(y_train)):     #when focusing on one of the 33 classifications, treat all others as 0 (binary)\n",
    "\n",
    "            if y1[j] == i:\n",
    "\n",
    "                y1[j] = 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                 y1[j] = 0\n",
    "\n",
    "        #print list(y1).count(1)\n",
    "\n",
    "        reg =  100000                              #regularization term\n",
    "        y1=y1.astype('int')\n",
    "        from sklearn import linear_model\n",
    "        sk_logreg_l2 = linear_model.LogisticRegression(C=1/reg,solver='lbfgs',fit_intercept=False)\n",
    "        sk_logreg_l2.fit(X_train,y1)\n",
    "\n",
    "        theta.append(np.ndarray.flatten(sk_logreg_l2.coef_))\n",
    "        \n",
    "    y_prob = []                                #you will have 33 probabilities from the 33 different hypothesis functions\n",
    "    for element in theta:\n",
    "        y_prob.append((sigmoid(X_test @ np.array(element))))\n",
    "        #print (np.array(y_prob).shape)\n",
    "\n",
    "\n",
    "    predictions = (np.argmax(y_prob,axis=0))       #for each example, find the theta that gives it the max probability\n",
    "    #print (dict2)\n",
    "   \n",
    "    new = []\n",
    "    for i in range(len(predictions)):\n",
    "        new.append(dict2[predictions[i]])\n",
    "            \n",
    "    df5 = pd.read_csv(\"weather_description.csv\")\n",
    "    data5 = df5[['Houston']]\n",
    "    data5 = np.array(data5)\n",
    "    y_train = data5[1:,:]\n",
    "\n",
    "    \n",
    "    total = 0\n",
    "    for i in range(len(new)):\n",
    "    \n",
    "        if str(y_train[i]) == new[i]: #compare the predicted output with the actual output\n",
    "        \n",
    "            total +=1\n",
    "            \n",
    "    return (float(total/(len(y_train))))\n",
    "\n",
    "\n",
    "print (\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2796561477945726\n"
     ]
    }
   ],
   "source": [
    "#print (final) \n",
    "print (find_theta(XX, XX, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
